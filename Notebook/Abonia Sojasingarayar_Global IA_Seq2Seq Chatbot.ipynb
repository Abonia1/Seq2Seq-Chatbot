{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> AI Chatbot Tensorflow Seq2seq Model</center>\n",
    "### <center> Deep Learning Project_Global IA</center> \n",
    "<b>Presented by</b>\t\t\t\t\t\t\t\t\n",
    "Abonia Sojasingarayar\t\t\t\t\t\t\n",
    "M2-Artificial Intelligence-IA school\t\t\n",
    "<b>Guided  by</b>\t\t\t\t\t\t\t\t\n",
    "Yacine Aslima\n",
    "Prof. AI-IA School\n",
    "\n",
    "## Credits and Motivation\n",
    "\n",
    "Siraj Raval - Founder of School of AI \n",
    "\n",
    "Andrew NG - Founder and CEO of Landing AI & Founder of deeplearning.ai\n",
    "\n",
    "Tensorflow Community\n",
    "\n",
    "Cornell University - For dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fundamentals:\n",
    "A <b>sequence</b> is an ordered list of symbols. For Ex:\n",
    "A sequence of webpages visited by a user, ordered by the time of access.\n",
    "A sequence of words or characters typed on a cellphone by a user, or in a text such as a book.\n",
    "A sequence of products bought by a customer in a retail store\n",
    "A sequence of proteins in bioinformatics\n",
    "A sequence of symptoms observed on a patient at a hospital\n",
    "\n",
    "Note : we consider that a sequence is a list of symbols and do not contain numeric values.  A sequence of numeric values is usually called a time-series rather than a sequence, and the task of predicting a time-series is called time-series forecasting. \n",
    "\n",
    "The task of <b>sequence prediction</b> consists of predicting the next symbol of a sequence based on the previously observed symbols. For example, if a user has visited some webpages A, B, C, in that order, one may want to predict what is the next webpage that will be visited by that user to prefetch the webpage.\n",
    "![title](DocImg/prediction.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN:\n",
    "\n",
    "Recurrent Neural Networks, or RNNs, were designed to work with sequence prediction problems. \n",
    "\n",
    "\n",
    "Recurrent means the output at the current time step becomes the input to the next time step. At each element of the sequence, the model considers not just the current input, but what it remembers about the preceding elements.\n",
    "![title](DocImg/rnn.png)\n",
    "\n",
    "![title](DocImg/rnns.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Types of RNN:\n",
    "\n",
    "One-to-Many: An observation as input mapped to a sequence with multiple steps as an output.\n",
    "Many-to-One: A sequence of multiple steps as input mapped to class or quantity prediction.\n",
    "Many-to-Many: A sequence of multiple steps as input mapped to a sequence with multiple steps as output.\n",
    "\n",
    "![title](DocImg/rnntype.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM:\n",
    "Long Short-Term Memory (LSTM) networks are a type of recurrent neural network capable of learning order dependence(long-term dependencies) in sequence prediction problems.\n",
    "This is a behavior required in complex problem domains like machine translation, speech recognition, and more.\n",
    "\n",
    "\n",
    "![title](DocImg/lstm.png)\n",
    "\n",
    "<i>Image Source: https://colah.github.io</i>\n",
    "\n",
    "\n",
    "## Seq2Seq LSTMs or RNN Encoder-Decoders:\n",
    "\n",
    "An “encoder” RNN reads the source sentence and transforms it into a rich fixed-length vector representation, which in turn in used as the initial hidden state of a “decoder” RNN that generates the target sentence. Here, we propose to follow this elegant recipe, replacing the encoder RNN by a deep convolution neural network (CNN). … it is natural to use a CNN as an image “encoder”, by first pre-training it for an image classification task and using the last hidden layer as an input to the RNN decoder that generates sentences!\n",
    "\n",
    "<i>Source: — Oriol Vinyals, et al., Show and Tell: A Neural Image Caption Generator, 2014</i>\n",
    "\n",
    "… an RNN Encoder–Decoder, consists of two recurrent neural networks (RNN) that act as an encoder and a decoder pair. The encoder maps a variable-length source sequence to a fixed-length vector, and the decoder maps the vector representation back to a variable-length target sequence.\n",
    "\n",
    "<i>Source: — Kyunghyun Cho, et al., Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation, 2014</i>\n",
    "\n",
    "![title](DocImg/encoderdecoder.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UBK-yfZdwsN5",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Import packages\n",
    "import warnings  \n",
    "with warnings.catch_warnings():  \n",
    "    warnings.filterwarnings(\"ignore\",category=FutureWarning)\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import re\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset : Cornell Movie Dialogs Corpus\n",
    "Description:\u000b",
    "\n",
    "This corpus contains a large metadata-rich collection of fictional conversations extracted from raw movie scripts.\n",
    "\n",
    "Link to download dataset : https://www.cs.cornell.edu/~cristian/Cornell_Movie-Dialogs_Corpus.html \t\t\t\t\t\n",
    "                            https://www.kaggle.com/rajathmc/cornell-moviedialog-corpus \u000b",
    "\u000b",
    "\n",
    "\n",
    "220,579 conversational exchanges between 10,292 pairs of movie characters\u000b",
    "\n",
    "\n",
    "Involves 9,035 characters from 617 movies\u000b",
    "\n",
    "\n",
    "In total 304,713 utterances\u000b",
    "\n",
    "\n",
    "Movie metadata included:\u000b",
    "\n",
    "    \n",
    "    Genres\u000b",
    "\n",
    "    Release year\u000b",
    "\n",
    "    IMDB rating\u000b",
    "\n",
    "    Number of IMDB votes\u000b",
    "\u000b",
    "\n",
    "\n",
    "Character metadata included:\u000b",
    "\n",
    "    \n",
    "    Gender (for 3,774 characters)\u000b",
    "\n",
    "    Position on movie credits (3,321 characters)\u000b",
    "\n",
    "\n",
    "README.txt (included) for details\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9UNOGC0Mwwlk"
   },
   "outputs": [],
   "source": [
    "#!wget http://www.cs.cornell.edu/~cristian/data/cornell_movie_dialogs_corpus.zip\n",
    "#!unzip cornell_movie_dialogs_corpus.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing or Data_Utils\n",
    "<b>Step 1:</b>\n",
    "Read from 'movie_conversations.txt'\n",
    "Create a list of [list of line_id's]\n",
    "\n",
    "Output Ex:['L194', 'L195', 'L196', 'L197']\n",
    "\n",
    "<b>Step 2:</b>\n",
    "Read from 'movie-lines.txt'\n",
    "Create a dictionary with ( key = line_id, value = text )\n",
    "\n",
    "Ex:\n",
    "They do not!\n",
    "\n",
    "They do to!\n",
    "\n",
    "I hope so.\n",
    "\n",
    "She okay?\n",
    "\n",
    "Let's go.\n",
    "\n",
    "Wow\n",
    "\n",
    "Okay -- you're gonna need to learn how to lie.\n",
    "\n",
    "No\n",
    "\n",
    "I'm kidding.  You know how sometimes you just become this \"persona\"?  And you don't know how to quit?\n",
    "\n",
    "Like my fear of wearing pastels?\n",
    "\n",
    "<b>Step 3:</b>\n",
    "Get lists of all conversations as Questions and Answers\n",
    " [questions]\n",
    " [answers]   \n",
    "\n",
    "Question and answers are come from same conversation.As because there will be a question with the response.\n",
    "\n",
    "Ex: For our first conversation\n",
    "\n",
    "Q Can we make this quick?  Roxanne Korrine and Andrew Barrett are having an incredibly horrendous public break- up on the quad.  Again.\n",
    "\n",
    "A Well, I thought we'd start with pronunciation, if that's okay with you.\n",
    "\n",
    "Q Well, I thought we'd start with pronunciation, if that's okay with you.\n",
    "\n",
    "A Not the hacking and gagging and spitting part.  Please.\n",
    "\n",
    "Q Not the hacking and gagging and spitting part.  Please.\n",
    "\n",
    "A Okay... then how 'bout we try out some French cuisine.  Saturday?  Night?\n",
    "\n",
    "\n",
    "<b>Step 4:</b>\n",
    "Clean Text:\n",
    "Text to lowercase \n",
    "Replacing certain words as follow:\n",
    "\n",
    "Ex:   \n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    \n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    \n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    \n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    \n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    \n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    \n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "\n",
    "<b>Step 5:</b>\n",
    "\n",
    "Filter out the Questions  and Answers that are too short/long\n",
    "\n",
    "Minimum&Maximum  length are 2&5\n",
    "\n",
    "<b>Step 6:</b>\n",
    "\n",
    "Get each word and its count  from filtered questions and answers in vocab dictionary\n",
    "\n",
    "Get each word and its count  from filtered questions and answers in Question and Answer vocab dictionary \n",
    "\n",
    "<b>Step 7:</b>\n",
    "\n",
    "Create vocabulary index with total number of words appear more than 2 times in vocab dictionary\n",
    "\t\n",
    "    6281 words which appear more appear more than 2 times \n",
    "\n",
    "<b>Step 8:</b>\n",
    "\n",
    "For each codes(<EOS>,<PAD>,<UNK><GO>) ,increment vocabulary index to 1 for each existing code \n",
    "Same for question and answer vocab.\n",
    "\t\n",
    "    Now vocab index will be 6285\n",
    "\n",
    "<b>Step 9:</b>\n",
    "\n",
    "Create index vocabulary from vocabulary index dictionary \n",
    "\t\n",
    "    index vocabulary dict_items([(0, 'what'), (1, 'good'), (2, 'stuff'), (3, 'she'), (4, 'okay'), (5, 'they'),......\n",
    "\t......., (6283, '<PAD>'), (6284, '<EOS>'), (6285, '<UNK>'), (6286, '<GO>')])\n",
    "\n",
    "<b>Step 10:</b>\n",
    "\n",
    "Add EOS tag at the end of each answer \t\n",
    "\tEx: the real you   -->   the real you <EOS>\n",
    "\n",
    "<b>Step 11:</b>\n",
    "\n",
    "Again filter out words in by comparing words in filtered question  and words in vocabulary index\n",
    "Do the same for filtered answer \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HrQk0g1kwsOE"
   },
   "outputs": [],
   "source": [
    "#get the conversation and movie data\n",
    "movie_line = \"../Datasets/cornell movie-dialogs corpus/movie_lines.txt\"\n",
    "movie_convo = \"../Datasets/cornell movie-dialogs corpus/movie_conversations.txt\"\n",
    "\n",
    "m_lines = open(movie_line , encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "c_lines = open(movie_convo , encoding='utf-8',errors='ignore').read().split('\\n')\n",
    "\n",
    "#get converastion lines\n",
    "convo_line = []\n",
    "for lines in c_lines:\n",
    "    _lines = lines.split(\" +++$+++ \")[-1][1:-1].replace(\"'\",\"\").replace(\" \",\"\")\n",
    "    convo_line.append(_lines.split(\",\"))\n",
    "\n",
    "#get movie lines\n",
    "id_line = {}\n",
    "for lines in m_lines:\n",
    "    _lines = lines.split(\" +++$+++ \")\n",
    "    if len(_lines) == 5:\n",
    "        id_line[_lines[0]] = _lines[4]\n",
    "        \n",
    "#Form questions and answers \n",
    "questions = []\n",
    "answers = []\n",
    "\n",
    "for line in convo_line:\n",
    "    for i in range(len(line) -1):\n",
    "        questions.append(id_line[line[i]])\n",
    "        answers.append(id_line[line[i+1]])\n",
    "        \n",
    "#Clean and replace improper words using regular expression\n",
    "def clean_text(text):\n",
    "    text = text.lower()\n",
    "    \n",
    "    text = re.sub(r\"i'm\", \"i am\", text)\n",
    "    text = re.sub(r\"he's\", \"he is\", text)\n",
    "    text = re.sub(r\"she's\", \"she is\", text)\n",
    "    text = re.sub(r\"it's\", \"it is\", text)\n",
    "    text = re.sub(r\"that's\", \"that is\", text)\n",
    "    text = re.sub(r\"what's\", \"that is\", text)\n",
    "    text = re.sub(r\"where's\", \"where is\", text)\n",
    "    text = re.sub(r\"how's\", \"how is\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"won't\", \"will not\", text)\n",
    "    text = re.sub(r\"can't\", \"cannot\", text)\n",
    "    text = re.sub(r\"n't\", \" not\", text)\n",
    "    text = re.sub(r\"n'\", \"ng\", text)\n",
    "    text = re.sub(r\"'bout\", \"about\", text)\n",
    "    text = re.sub(r\"'til\", \"until\", text)\n",
    "    text = re.sub(r\"  \",\"\",text)\n",
    "    text = re.sub(r\"[-()\\\"#/@;:<>{}`+=~|.!?,]\", \"\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "clean_questions = []\n",
    "clean_answers = []\n",
    "\n",
    "for q in questions:\n",
    "    clean_questions.append(clean_text(q))\n",
    "for a in answers:\n",
    "    clean_answers.append(clean_text(a))\n",
    "    \n",
    "#get the min and max length of sentence need to be used\n",
    "max_length = 5\n",
    "min_length = 2\n",
    "\n",
    "codes = ['<PAD>','<EOS>','<UNK>','<GO>']\n",
    "\n",
    "\n",
    "\n",
    "short_questions_temp = []\n",
    "short_answers_temp = []\n",
    "\n",
    "i = 0\n",
    "for question in clean_questions:\n",
    "    if len(question.split()) >= min_length and len(question.split()) <= max_length:\n",
    "        short_questions_temp.append(question)\n",
    "        short_answers_temp.append(clean_answers[i])\n",
    "    i += 1\n",
    "\n",
    "# Filter out the answers that are too short/long\n",
    "shorted_q = []\n",
    "shorted_a = []\n",
    "\n",
    "i = 0\n",
    "for answer in short_answers_temp:\n",
    "    if len(answer.split()) >= min_length and len(answer.split()) <= max_length:\n",
    "        shorted_a.append(answer)\n",
    "        shorted_q.append(short_questions_temp[i])\n",
    "    i += 1\n",
    "   \n",
    "  \n",
    "\n",
    "#Get the count of words from filtered questions and answers  \n",
    "vocab = {}\n",
    "\n",
    "for question in shorted_q:\n",
    "    for words in question.split():\n",
    "        if words not in vocab:\n",
    "            vocab[words] = 1\n",
    "        else:\n",
    "            vocab[words] +=1\n",
    "for answer in shorted_a:\n",
    "    for words in answer.split():\n",
    "        if words not in vocab:\n",
    "            vocab[words] = 1\n",
    "        else:\n",
    "            vocab[words] +=1\n",
    "            \n",
    "questions_vocabs = {}\n",
    "for answer in shorted_q:\n",
    "    for words in answer.split():\n",
    "        if words not in questions_vocabs:\n",
    "            questions_vocabs[words] = 1\n",
    "        else:\n",
    "            questions_vocabs[words] +=1\n",
    "            \n",
    "answers_vocabs = {}\n",
    "for answer in shorted_a:\n",
    "    for words in answer.split():\n",
    "        if words not in answers_vocabs:\n",
    "            answers_vocabs[words] = 1\n",
    "        else:\n",
    "            answers_vocabs[words] +=1\n",
    "            \n",
    "#total number of words appear more than 2 times\n",
    "vocabs_to_index = {}\n",
    "threshold = 2\n",
    "word_num = 0\n",
    "for word, count in vocab.items():\n",
    "    if count >= threshold:\n",
    "        vocabs_to_index[word] = word_num\n",
    "        word_num += 1\n",
    "\n",
    "#add words in codes in the text and  increment vocab index to 1 for each existing code \n",
    "#same for question and answer vocab.6281 in vocab dict and now 6286        \n",
    "for code in codes:\n",
    "    vocabs_to_index[code] = len(vocabs_to_index)+1\n",
    "    \n",
    "for code in codes:\n",
    "    questions_vocabs[code] = len(questions_vocabs)+1\n",
    "\n",
    "for code in codes:\n",
    "    answers_vocabs[code] = len(answers_vocabs)+1\n",
    "\n",
    "#Convert index vocab to vocab index   \n",
    "index_to_vocabs = {v_i: v for v, v_i in vocabs_to_index.items()}\n",
    "\n",
    "#Add <EOS> to the end of all the answer in such a way model can learn the the sentence comes to the end \n",
    "for i in range(len(shorted_a)):\n",
    "  shorted_a[i] += ' <EOS>'\n",
    "  \n",
    "#Get the question and with code <UNK> for the words which are not in vocab to index\n",
    "#ex:'nowhere hi daddy <EOS> ' to '[6285, 179, 22, 6284]' as it doesnt find the word 'nowhere' in the vocabulary index dictionary\n",
    "\n",
    "questions_int = []\n",
    "for question in shorted_q:\n",
    "    ints = []\n",
    "    for word in question.split():\n",
    "        if word not in vocabs_to_index:\n",
    "            ints.append(vocabs_to_index['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocabs_to_index[word])\n",
    "    questions_int.append(ints)\n",
    "    \n",
    "answers_int = []\n",
    "for answer in shorted_a:\n",
    "    ints = []\n",
    "    for word in answer.split():\n",
    "        if word not in vocabs_to_index:\n",
    "            ints.append(vocabs_to_index['<UNK>'])\n",
    "        else:\n",
    "            ints.append(vocabs_to_index[word])\n",
    "    answers_int.append(ints)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "X0o_V2ofuDrk",
    "outputId": "6c12c65f-7cdd-4bad-f0de-b2ee693b1b63"
   },
   "outputs": [],
   "source": [
    "for code in codes:\n",
    "  print(vocabs_to_index[code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration:\n",
    "<b>source_vocab_size</b> is the size of questions vocabulary dictionary.In our case:9611\n",
    "\n",
    "<b>target_vocab_size</b> is the size of answers vocabulary dictionary.In our case:9636\n",
    "\n",
    "<b>vocab size</b> is the length of vocabulary index dictionary in our case its 6286\n",
    "\n",
    "The <b>learning rate</b> is a hyperparameter that controls how much to change the model in response to the estimated error each time the model weights are updated. Choosing the learning rate is challenging as a value too small may result in a long training process that could get stuck, whereas a value too large may result in learning a sub-optimal set of weights too fast or an unstable training process.\n",
    "\n",
    "<b>learning Rate decay:</b>An alternative to using a fixed learning rate is to instead vary the learning rate over the training process.\n",
    "The way in which the learning rate changes over time (training epochs) is referred to as the learning rate schedule or learning rate decay.\n",
    "\n",
    "The <b>keep_prob</b> value is used to control the dropout rate used when training the network. Essentially, it means that each connection between layers (in this case between the last densely connected layer and the readout layer) will only be used with probability 0.5 when training. This reduces overfitting.\n",
    "\n",
    "The <b>batch size</b> is a hyperparameter that defines the number of samples to work through before updating the internal model parameters.\n",
    "\n",
    "The number of <b>epochs</b> is a hyperparameter that defines the number times that the learning algorithm will work through the entire training dataset.\n",
    "\n",
    "<b>Working Example:</b> Assume you have a dataset with 200 samples (rows of data) and you choose a batch size of 5 and 1,000 epochs.\n",
    "\n",
    "This means that the dataset will be divided into 40 batches, each with five samples. The model weights will be updated after each batch of five samples.\n",
    "\n",
    "This also means that one epoch will involve 40 batches or 40 updates to the model.\n",
    "\n",
    "With 1,000 epochs, the model will be exposed to or pass through the whole dataset 1,000 times. That is a total of 40,000 batches during the entire training process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lHBhX_UMwsOH"
   },
   "outputs": [],
   "source": [
    "target_vocab_size = len(answers_vocabs)\n",
    "source_vocab_size = len(questions_vocabs)\n",
    "vocab_size = len(index_to_vocabs)+1\n",
    "embed_size = 1024\n",
    "rnn_size = 1024\n",
    "batch_size = 32\n",
    "num_layers =  3\n",
    "learning_rate = 0.001\n",
    "learning_rate_decay = 0.99\n",
    "min_lr = 0.0001\n",
    "#keep_prob = 0.5\n",
    "epochs=50\n",
    "DISPLAY_STEP=30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM and DropoutWrapper\n",
    "\n",
    "<b>class LSTMCell:</b> Long short-term memory unit (LSTM) recurrent network cell.\n",
    "\n",
    "rnn_size: int, The number of units in the LSTM cell.\n",
    "\n",
    "reuse:Python boolean describing whether to reuse variables in an existing scope. If not True, and the existing scope already has the given variables, an error is raised.\n",
    "\n",
    "\n",
    "<b>class DropoutWrapper</b>:Operator adding dropout to inputs and outputs of the given cell.\n",
    "\n",
    "cell: an RNNCell, a projection to output_size is added to it.\n",
    "input_keep_prob: unit Tensor or float between 0 and 1, input keep probability; if it is constant and 1, no input dropout will be added.\n",
    "\n",
    "output_keep_prob: unit Tensor or float between 0 and 1, output keep probability; if it is constant and 1, no output dropout will be added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e2Dg_dItwsOJ"
   },
   "outputs": [],
   "source": [
    "def lstm(rnn_size, keep_prob,reuse=False):\n",
    "    lstm =tf.nn.rnn_cell.LSTMCell(rnn_size,reuse=reuse)\n",
    "    drop =tf.nn.rnn_cell.DropoutWrapper(lstm, output_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention Mechanism:\n",
    "A neural network is considered to be an effort to mimic human brain actions in a simplified manner. Attention Mechanism is also an attempt to implement the same action of selectively concentrating on a few relevant things in input, while ignoring others in deep neural networks when producing output\n",
    "\n",
    "### Bahdanau Attention\n",
    "\n",
    "Bahdanau et al (2015) came up with a simple but elegant idea where they suggested that not only can all the input words be taken into account in the context vector, but relative importance should also be given to each one of them.\n",
    "\n",
    "![title](DocImg/Battention.jpg)\n",
    "\n",
    "<center>Overall process for Bahdanau Attention seq2seq model</center>\n",
    "\n",
    "\n",
    "The first type of Attention, commonly referred to as Additive Attention, came from a paper by Dzmitry Bahdanau, which explains the less-descriptive original name. The paper aimed to improve the sequence-to-sequence model in machine translation by aligning the decoder with the relevant input sentences and implementing Attention. The entire step-by-step process of applying Attention in Bahdanau’s paper is as follows:\n",
    "\n",
    "1;Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input sequence\n",
    "\n",
    "2.Calculating Alignment Scores between the previous decoder hidden state and each of the encoder’s hidden states are calculated (Note: The last encoder hidden state can be used as the first hidden state in the decoder)\n",
    "\n",
    "3.Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed\n",
    "\n",
    "4.Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector\n",
    "\n",
    "5.Decoding the Output - the context vector is concatenated with the previous decoder output and fed into the Decoder RNN for that time step along with the previous decoder hidden state to produce a new output\n",
    "\n",
    "6.The process (steps 2-5) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length\n",
    "\n",
    "Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio. \"Neural Machine Translation by Jointly Learning to Align and Translate.\" ICLR 2015. https://arxiv.org/abs/1409.0473\n",
    "\n",
    "The second is the normalized form. This form is inspired by the weight normalization article:\n",
    "\n",
    "Tim Salimans, Diederik P. Kingma. \"Weight Normalization: A Simple Reparameterization to Accelerate Training of Deep Neural Networks.\" https://arxiv.org/abs/1602.07868\n",
    "\n",
    "<b>Class BahdanauAttention</b>\n",
    "\n",
    "Args:\n",
    "\n",
    "num_units: The depth of the query mechanism.\n",
    "\n",
    "memory: The memory to query; usually the output of an RNN encoder. This tensor should be shaped [batch_size, max_time, ...]. \n",
    "\n",
    "memory_sequence_length (optional): Sequence lengths for the batch entries in memory. If provided, the memory tensor rows are masked with zeros for values past the respective sequence lengths.\n",
    "\n",
    "normalize: Python boolean. Whether to normalize the energy term.\n",
    "\n",
    "probability_fn: (optional) A callable. Converts the score to probabilities. The default is tf.nn.softmax. Other options include tf.contrib.seq2seq.hardmax and tf.contrib.sparsemax.sparsemax. Its signature should be: probabilities = \n",
    "\n",
    "probability_fn(score).\n",
    "score_mask_value: (optional): The mask value for score before passing into probability_fn. The default is -inf. Only used if \n",
    "\n",
    "memory_sequence_length is not None.\n",
    "\n",
    "<b>class AttentionWrapper:</b>Wraps another RNNCell with attention.\n",
    "\n",
    "dec_cell: An instance of RNNCell.\n",
    "\n",
    "attention_mechanism: A list of AttentionMechanism instances or a single instance.\n",
    "\n",
    "attention_layer_size: A list of Python integers or a single Python integer, the depth of the attention (output) layer(s). If None (default), use the context as attention at each time step. Otherwise, feed the context and cell output into the attention layer to generate attention at each time step. If attention_mechanism is a list, attention_layer_size must be a list of the same length. If attention_layer is set, this must be None. If attention_fn is set, it must guaranteed that the outputs of attention_fn also meet the above requirements.\n",
    "\n",
    "source: 1. https://www.analyticsvidhya.com/blog/2019/11/comprehensive-guide-attention-mechanism-deep-learning/\n",
    "   2. https://blog.floydhub.com/attention-mechanism/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zTsNF75pwsOL"
   },
   "outputs": [],
   "source": [
    "def attention(rnn_size,encoder_outputs,target_sequence_length,dec_cell):\n",
    "    attention_mechanism = tf.contrib.seq2seq.BahdanauAttention(rnn_size*2,encoder_outputs,\n",
    "                                                                   memory_sequence_length=target_sequence_length)\n",
    "    attention_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell, attention_mechanism,\n",
    "                                                             attention_layer_size=rnn_size/2)\n",
    "    return attention_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b>placeholder</b> is used for feeding external data into a Tensorflow computation (stuff outside the graph). Here's some documentation: https://www.tensorflow.org/versions/r0.10/how_tos/reading_data/#feeding\n",
    "\n",
    "TensorFlow's feed mechanism lets you inject data into any Tensor in a computation graph. A python computation can thus feed data directly into the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BcGIjHaBwsON"
   },
   "outputs": [],
   "source": [
    "input_data = tf.placeholder(tf.int32, [None, None],name='input')\n",
    "target_data = tf.placeholder(tf.int32, [None, None],name='target')\n",
    "input_data_len = tf.placeholder(tf.int32,[None],name='input_len')\n",
    "target_data_len = tf.placeholder(tf.int32,[None],name='target_len')\n",
    "lr_rate = tf.placeholder(tf.float32,name='lr')\n",
    "keep_prob = tf.placeholder(tf.float32,name='keep_prob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Encoder</center>\n",
    "The LSTM network can be organized into an architecture called the Encoder-Decoder LSTM that allows the model to be used to both support variable length input sequences and to predict or output variable length output sequences.\n",
    "\n",
    "This architecture is the basis for many advances in complex sequence prediction problems such as speech recognition and text translation.\n",
    "\n",
    "In this architecture, an <b>encoder</b> LSTM model reads the input sequence step-by-step. After reading in the entire input sequence, the hidden state or output of this model represents an internal learned representation of the entire input sequence as a fixed-length vector. This vector is then provided as an input to the <b>decoder</b> model that interprets it as each step in the output sequence is generated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A <b> tf.variable</b> is used to store state in graph. It requires an initial value. One use case could be representing weights of a neural network or something similar. Here's documentation: (https://www.tensorflow.org/api_docs/python/tf/Variable)\n",
    "\n",
    "A variable maintains state in the graph across calls to run(). You add a variable to the graph by constructing an instance of the class Variable.\n",
    "\n",
    "The Variable() constructor requires an initial value for the variable, which can be a Tensor of any type and shape. The initial value defines the type and shape of the variable. After construction, the type and shape of the variable are fixed. The value can be changed using one of the assign methods.\n",
    "\n",
    "<b>random_uniform</b>:Outputs random values from a uniform distribution. Generate a random tensor in TensorFlow so that you can use it and maintain it for further use even if you call session run multiple times\n",
    "\n",
    "<b>encoder_embeddings:</b> holds the random value of tensors with shape of [source_vocab_size, embed_size]\n",
    "\n",
    "it output : <tf.Variable 'Variable:0' shape=(9611, 128) dtype=float32_ref>\n",
    "\n",
    "So [9611, 128] this is the shape of embedding matrix which xill be used for embedding lookup\n",
    "\n",
    "### Word Embedding \n",
    "\n",
    "Word embedding is one of the most popular representation of document vocabulary. It is capable of capturing context of a word in a document, semantic and syntactic similarity, relation with other words, etc.\n",
    "\n",
    "Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text.\n",
    "\n",
    "Take a look at this example – sentence=” Word Embeddings are Word converted into numbers ”\n",
    "\n",
    "A word in this sentence may be “Embeddings” or “numbers ” etc.\n",
    "\n",
    "A dictionary may be the list of all unique words in the sentence. So, a dictionary may look like – [‘Word’,’Embeddings’,’are’,’Converted’,’into’,’numbers’]\n",
    "\n",
    "A vector representation of a word may be a one-hot encoded vector where 1 stands for the position where the word exists and 0 everywhere else. The vector representation of “numbers” in this format according to the above dictionary is [0,0,0,0,0,1] and of converted is[0,0,0,1,0,0].\n",
    "![title](DocImg/one-hot.jpg)            ![title](DocImg/wordembed.jpg)\n",
    "\n",
    "![title](DocImg/one-hot to wordembed.jpg)\n",
    "\n",
    "<center><i>Source:https://confengine.com/odsc-india-2019/proposal/10176/sequence-to-sequence-learning-with-encoder-decoder-neural-network-models</i></center>\n",
    "\n",
    "### Word2Vec\n",
    "\n",
    "Word2Vec is one of the most popular technique to learn word embeddings using shallow neural network.\n",
    "\n",
    "Mitolov introduced word2vec to the NLP community. These methods were prediction based in the sense that they provided probabilities to the words and proved to be state of the art for tasks like word analogies and word similarities. They were also able to achieve tasks like King -man +woman = Queen, which was considered a result almost magical. So let us look at the word2vec model used as of today to generate word vectors.\n",
    "\n",
    "Word2vec is not a single algorithm but a combination of two techniques – CBOW(Continuous bag of words) and Skip-gram model. Both of these are shallow neural networks which map word(s) to the target variable which is also a word(s). \n",
    "\n",
    "<i>source:https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/</i>\n",
    "\n",
    "### <b>Class embedding_lookup:</b>\n",
    "embedding_lookup function retrieves rows of the params tensor. The behavior is similar to using indexing with arrays in numpy.\n",
    "\n",
    "![title](DocImg/embed_lookup2.jpg)\n",
    "\n",
    "![title](DocImg/embed_lookup.jpg)\n",
    "\n",
    "<i>Note:In above diagram 100 is the row size of embedding matrix but in our case its 9611 i.e our vocabulary size of questions</i>\n",
    "\n",
    "source:Stackoverflow\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "z7TNblfPwsOQ",
    "outputId": "c87b1f0d-00e1-4f90-ce76-75356864c0af"
   },
   "outputs": [],
   "source": [
    "encoder_embeddings = tf.Variable(tf.random_uniform([source_vocab_size, embed_size], -1, 1))\n",
    "encoder_embedded = tf.nn.embedding_lookup(encoder_embeddings, input_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bidirectional_dynamic_rnn\n",
    "\n",
    "If you want to have have multiple layers that pass the information backward or forward in time, there are two ways how to design this. Assume the forward layer consists of two layers F1, F2 and the backword layer consists of two layers B1, B2.\n",
    "\n",
    "If you use tf.nn.bidirectional_dynamic_rnn the model will look like this (time flows from left to right):\n",
    "\n",
    "![title](DocImg/bidir_rnn.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "id": "Gag764m8wsOa",
    "outputId": "e792c91a-5ee8-432e-c09c-ca76978fb14c"
   },
   "outputs": [],
   "source": [
    "stacked_cells = lstm(rnn_size, keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "colab_type": "code",
    "id": "LVt0hjBzwsOg",
    "outputId": "41ea5d68-dfc4-48c4-c010-4e13ec80e85e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "((encoder_fw_outputs,encoder_bw_outputs),\n",
    " (encoder_fw_final_state,encoder_bw_final_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=stacked_cells, \n",
    "                                                                 cell_bw=stacked_cells, \n",
    "                                                                 inputs=encoder_embedded, \n",
    "                                                                 sequence_length=input_data_len, \n",
    "                                                                 dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QVKq1wurwsOm"
   },
   "outputs": [],
   "source": [
    "encoder_outputs = tf.concat((encoder_fw_outputs,encoder_bw_outputs),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "XvdjQBE6wsOp",
    "outputId": "faa12e85-2410-461b-c558-252534f50e59"
   },
   "outputs": [],
   "source": [
    "encoder_outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <b>class LSTMStateTuple</b>\n",
    "Tuple used by LSTM Cells for state_size, zero_state, and output state.\n",
    "\n",
    "Stores two elements: (c, h), in that order. Where c is the hidden state and h is the output.\n",
    "\n",
    "Only used when state_is_tuple=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QsFapqt-wsOt"
   },
   "outputs": [],
   "source": [
    "encoder_state_c = tf.concat((encoder_fw_final_state.c,encoder_bw_final_state.c),1)\n",
    "encoder_state_h = tf.concat((encoder_fw_final_state.h,encoder_bw_final_state.h),1)\n",
    "encoder_states = tf.nn.rnn_cell.LSTMStateTuple(c=encoder_state_c,h=encoder_state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "8qEgz1PdwsOv",
    "outputId": "28b5617f-64ef-4781-9df0-0e9077ac606e"
   },
   "outputs": [],
   "source": [
    "encoder_states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Decoder</center>\n",
    "### <b>strided_slice</b>\n",
    "To a first order, this operation extracts a slice of size <b>end - begin</b> from a tensor input starting at the location specified by begin. The slice continues by adding stride to the begin index until all dimensions are not less than end. Note that components of stride can be negative, which causes a reverse slice.\n",
    "\n",
    "<b>ex:</b>\n",
    "\n",
    "begin = [1, 0, 0] and end = [2, 1, 3]. Also, all the strides are 1. Work your way backwards, from the last dimension.\n",
    "\n",
    "Start with element [1,0,0]. Now increase the last dimension only by its stride amount, giving you [1,0,1]. Keep doing this until you reach the limit. Something like [1,0,2], [1,0,3] (end of the loop). Now in your next iteration, start by incrementing the second to last dimension and resetting the last dimension, [1,1,0]. Here the second to last dimension is equal to end[1], so move to the first dimension (third to last) and reset the rest, giving you [2,0,0]. Again you are at the first dimension’s limit, so quit the loop.\n",
    "\n",
    "In our case: input=target_data,begin=[0,0],end=[batchsize,-1], stride[1,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ecXG9168wsOz"
   },
   "outputs": [],
   "source": [
    "main = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1])\n",
    "decoder_input = tf.concat([tf.fill([batch_size, 1],vocabs_to_index['<GO>']), main], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UAvG9wW1wsO3"
   },
   "outputs": [],
   "source": [
    "#sam process as followed in encoder embedding and lookups\n",
    "decoder_embeddings = tf.Variable(tf.random_uniform([target_vocab_size, embed_size], -1, 1))\n",
    "dec_cell_inputs = tf.nn.embedding_lookup(decoder_embeddings, decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B3N5JtvZwsO8"
   },
   "outputs": [],
   "source": [
    "dec_cell = lstm(rnn_size*2,keep_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "nfmoMirUwsO_",
    "outputId": "13b6928a-1b8b-43a4-ed8e-9331978bc2b8"
   },
   "outputs": [],
   "source": [
    "dec_cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dense layer\n",
    "\n",
    "Single output layer\n",
    "\n",
    "target_vocab_size=dimensionality of the output space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZyZQ2NR6wsPC"
   },
   "outputs": [],
   "source": [
    "#output layer for decoder\n",
    "dense_layer = tf.layers.Dense(target_vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class TrainingHelper\n",
    "\n",
    "A helper for use during training. Only reads inputs.\n",
    "\n",
    "Returned sample_ids are the argmax of the RNN output logits\n",
    "\n",
    "dec_cell_inputs=>inputs: A (structure of) input tensors.\n",
    "\n",
    "target_data_len=>sequence_length: An int32 vector tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "TR4iLAuDwsPJ",
    "outputId": "a9baafc5-193e-4cc2-9762-31c4174f76e0"
   },
   "outputs": [],
   "source": [
    "train_helper = tf.contrib.seq2seq.TrainingHelper(dec_cell_inputs, target_data_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cell state initializer\n",
    "<b>attention_cell.zero_state</b>\n",
    "\n",
    "The two are different things. state_is_tuple is used on LSTM cells because the state of LSTM cells is a tuple. cell.zero_state is the initializer of the state for all RNN cells.\n",
    "\n",
    "You will generally prefer cell.zero_state function as it will initialize the required state class depending on whether state_is_tuple is true or not.\n",
    "\n",
    "See this GitHub issue where you can see the cell.zero_state recommended - \"use the zero_state function on the cell object\".\n",
    "\n",
    "Another reason why you may want cell.zero_state is because it is agnostic of the type of the cell (LSTM, GRU, RNN) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5FJBKKGywsPN"
   },
   "outputs": [],
   "source": [
    "attention_cell = attention(rnn_size,encoder_outputs,target_data_len,dec_cell)\n",
    "state = attention_cell.zero_state(dtype=tf.float32, batch_size=batch_size)\n",
    "state = state.clone(cell_state=encoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class BasicDecoder\n",
    "Basic sampling decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BMXJ5P_9wsPP"
   },
   "outputs": [],
   "source": [
    "decoder_train = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=train_helper, \n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Class dynamic_decode\n",
    "Perform dynamic decoding with decoder.\n",
    "\n",
    "Calls initialize() once and step() repeatedly on the Decoder object.\n",
    "\n",
    "impute_finished: Python boolean. If True, then states for batch entries which are marked as finished get copied through and the corresponding outputs get zeroed out. This causes some slowdown at each time step, but ensures that the final state and outputs have the correct values and that backprop ignores time steps that were marked as finished.\n",
    "\n",
    "maximum_iterations: maximum allowed number of decoding steps. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Vcsu2S0wsPR"
   },
   "outputs": [],
   "source": [
    "outputs_train, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_train, \n",
    "                                                  impute_finished=True, \n",
    "                                                  maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Greedy Search\n",
    "A simple approximation is to use a greedy search that selects the most likely word at each step in the output sequence.\n",
    "\n",
    "This approach has the benefit that it is very fast, but the quality of the final output sequences may be far from optimal.\n",
    "\n",
    "ex:define a sequence of 10 words over a vocab of 5 words\n",
    "\n",
    "\n",
    "        data = [[0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "               [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "               [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "               [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "               [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "               [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "               [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "               [0.5, 0.4, 0.3, 0.2, 0.1],\n",
    "               [0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "               [0.5, 0.4, 0.3, 0.2, 0.1]]\n",
    "     \n",
    "After applying greedy decoder that mapped back to words in the vocabulary. \n",
    "\n",
    "Return the words which have maximum probability at athose time \n",
    "\n",
    "               [4, 0, 4, 0, 4, 0, 4, 0, 4, 0]\n",
    "\n",
    "        \n",
    "### Class GreedyEmbeddingHelper\n",
    "\n",
    "A helper for use during inference.\n",
    "\n",
    "Uses the argmax of the output (treated as logits) and passes the result through an embedding layer to get the next input.\n",
    "\n",
    "embedding: A callable that takes a vector tensor of ids (argmax ids), or the params argument for embedding_lookup. The returned tensor will be passed to the decoder input.\n",
    "\n",
    "start_tokens: int32 vector shaped [batch_size], the start tokens.\n",
    "\n",
    "end_token: int32 scalar, the token that marks end of decoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jf02Hu8ewsPV"
   },
   "outputs": [],
   "source": [
    "infer_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(decoder_embeddings, \n",
    "                                                          tf.fill([batch_size], vocabs_to_index['<GO>']), \n",
    "                                                          vocabs_to_index['<EOS>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KUcM6qQOwsPX"
   },
   "outputs": [],
   "source": [
    "decoder_infer = tf.contrib.seq2seq.BasicDecoder(cell=attention_cell, helper=infer_helper, \n",
    "                                                  initial_state=state,\n",
    "                                                  output_layer=dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pq0kpF-BwsPY"
   },
   "outputs": [],
   "source": [
    "outputs_infer, _, _ = tf.contrib.seq2seq.dynamic_decode(decoder_infer, impute_finished=True,\n",
    "                                                          maximum_iterations=tf.reduce_max(target_data_len))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class identity\n",
    "Return a tensor with the same shape and contents as input.\n",
    "\n",
    "input=outputs_train.rnn_output for logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7e00xllwsPb"
   },
   "outputs": [],
   "source": [
    "training_logits = tf.identity(outputs_train.rnn_output, name='logits')\n",
    "inference_logits = tf.identity(outputs_infer.sample_id, name='predictions')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Padding and Masking\n",
    "Now that all samples have a uniform length, the model must be informed that some part of the data is actually padding and should be ignored. That mechanism is <b>masking</b>.\n",
    "ex:\n",
    "\n",
    "    [\n",
    "      [\"The\", \"weather\", \"will\", \"be\", \"nice\", \"tomorrow\"],\n",
    "      [\"How\", \"are\", \"you\", \"doing\", \"today\"],\n",
    "      [\"Hello\", \"world\", \"!\"]\n",
    "    ]\n",
    "can also be \n",
    "\n",
    "    [\n",
    "      [83, 91, 1, 645, 1253, 927],\n",
    "      [73, 8, 3215, 55, 927],\n",
    "      [71, 1331, 4231]\n",
    "    ]\n",
    "Afetr padding\n",
    "\n",
    "    [[  83   91    1  645 1253  927]\n",
    "     [  73    8 3215   55  927    0]\n",
    "     [ 711  632   71    0    0    0]]\n",
    "     \n",
    "After masking\n",
    "\n",
    "    tf.Tensor(\n",
    "    [[ True  True  True  True  True  True]\n",
    "     [ True  True  True  True  True False]\n",
    "     [ True  True  True False False False]], shape=(3, 6), dtype=bool)\n",
    "### Class sequence_mask\n",
    "When using the Functional API or the Sequential API, a mask generated by an Embedding or Masking layer will be propagated through the network for any layer that is capable of using them (for example, RNN layers). \n",
    "\n",
    "Note that in the call method of a subclassed model or layer, masks aren't automatically propagated, so you will need to manually pass a mask argument to any layer that needs one. \n",
    "\n",
    "Returns a mask tensor representing the first N positions of each cell.\n",
    "\n",
    "\n",
    "If lengths has shape [d_1, d_2, ..., d_n] the resulting tensor mask has dtype and \n",
    "\n",
    "shape [d_1, d_2, ..., d_n, maxlen], with\n",
    "\n",
    "mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])\n",
    "\n",
    "Examples:\n",
    "tf.sequence_mask([1, 3, 2], 5)  \n",
    "                                # [[True, False, False, False, False],\n",
    "                                #  [True, True, True, False, False],\n",
    "                                #  [True, True, False, False, False]]\n",
    "\n",
    "tf.sequence_mask([[1, 3],[2,0]])  \n",
    "                                  # [[[True, False, False],\n",
    "                                  #   [True, True, True]],\n",
    "                                  #  [[True, True, False],\n",
    "                                  #   [False, False, False]]]\n",
    "                                  \n",
    "length=target_data_len\n",
    "\n",
    "max_len=tf.reduce_max(target_data_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pCczbxr-wsPd"
   },
   "outputs": [],
   "source": [
    "masks = tf.sequence_mask(target_data_len, tf.reduce_max(target_data_len), dtype=tf.float32, name='masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class sequence_loss\n",
    "Weighted cross-entropy loss for a sequence of logits.\n",
    "\n",
    "logits: A Tensor of shape [batch_size, sequence_length, num_decoder_symbols] and dtype float. The logits correspond to the prediction across all classes at each timestep.\n",
    "\n",
    "targets: A Tensor of shape [batch_size, sequence_length] and dtype int. The target represents the true class at each timestep.\n",
    "\n",
    "weights: A Tensor of shape [batch_size, sequence_length] and dtype float. weights constitutes the weighting of each prediction in the sequence. When using weights as masking, set all valid timesteps to 1 and all padded timesteps to 0, e.g. a mask returned by tf.sequence_mask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "txKobnLEwsPf"
   },
   "outputs": [],
   "source": [
    "cost = tf.contrib.seq2seq.sequence_loss(training_logits,target_data,masks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Adam Optimizer</center>\n",
    "\n",
    "Adam is different to classical stochastic gradient descent.\n",
    "\n",
    "Stochastic gradient descent maintains a single learning rate (termed alpha) for all weight updates and the learning rate does not change during training.\n",
    "\n",
    "A learning rate is maintained for each network weight (parameter) and separately adapted as learning unfolds.\n",
    "\n",
    "The method computes individual adaptive learning rates for different parameters from estimates of first and second moments of the gradients.\n",
    "\n",
    "The authors describe Adam as combining the advantages of two other extensions of stochastic gradient descent. Specifically:\n",
    "\n",
    "<b>Adaptive Gradient Algorithm (AdaGrad)</b> that maintains a per-parameter learning rate that improves performance on problems with sparse gradients (e.g. natural language and computer vision problems).\n",
    "\n",
    "<b>Root Mean Square Propagation (RMSProp)</b> that also maintains per-parameter learning rates that are adapted based on the average of recent magnitudes of the gradients for the weight (e.g. how quickly it is changing). This means the algorithm does well on online and non-stationary problems (e.g. noisy).\n",
    "Adam realizes the benefits of both AdaGrad and RMSProp.\n",
    "\n",
    "Instead of adapting the parameter learning rates based on the average first moment (the mean) as in RMSProp, Adam also makes use of the average of the second moments of the gradients (the uncentered variance).\n",
    "\n",
    "Adam is a replacement optimization algorithm for stochastic gradient descent for training deep learning models.\n",
    "Adam combines the best properties of the AdaGrad and RMSProp algorithms to provide an optimization algorithm that can handle sparse gradients on noisy problems.\n",
    "Adam is relatively easy to configure where the default configuration parameters do well on most problems.\n",
    "\n",
    "alpha (in our case lr_rate): Also referred to as the learning rate or step size. The amount that the weights are updated during training is referred to as the step size or the “learning rate.”\n",
    "\n",
    "\n",
    "The proportion that weights are updated (e.g. 0.001). Larger values (e.g. 0.3) results in faster initial learning before the rate is updated. Smaller values (e.g. 1.0E-5) slow learning right down during training\n",
    "\n",
    "source:https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wsg4xpVIwsPi"
   },
   "outputs": [],
   "source": [
    "optimizer = tf.train.AdamOptimizer(lr_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Gradient Clipping\n",
    "Gradient clipping is a technique to prevent exploding gradients in very deep networks, usually in recurrent neural networks. A neural network is a learning algorithm, also called neural network or neural net, that uses a network of functions to understand and translate data input into a specific output. This type of learning algorithm is designed based on the way neurons function in the human brain. There are many ways to compute gradient clipping, but a common one is to rescale gradients so that their norm is at most a particular value. With gradient clipping, pre-determined gradient threshold be introduced, and  then gradients norms that exceed this threshold are scaled down to match the norm.  This prevents any gradient to have norm greater than the threshold and thus the gradients are clipped.  There is an introduced bias in the resulting values from the gradient, but gradient clipping can keep things stable. \n",
    "\n",
    "<b>Why is this Useful?</b>\n",
    "Training recurrent neural networks can be very difficult. Two common issues with training recurrent neural networks are vanishing gradients and exploding gradients. Exploding gradients can occur when the gradient becomes too large and error gradients accumulate, resulting in an unstable network. Vanishing gradients can happen when optimization gets stuck at a certain point because the gradient is too small to progress. Gradient clipping can prevent these issues in the gradients that mess up the parameters during training.\n",
    "\n",
    "![title](DocImg/gradclip.png)\n",
    "\n",
    "<center><i>Source:https://deepai.org/</i></center>\n",
    "### clip_by_value\n",
    "Clips tensor values to a specified min and max.\n",
    "\n",
    "Compute gradients of loss for the variables in var_list.\n",
    "\n",
    "This is the first part of minimize(). It returns a list of (gradient, variable) pairs where \"gradient\" is the gradient for \"variable\". Note that \"gradient\" can be a Tensor, an IndexedSlices, or None if there is no gradient for the given variable.\n",
    "\n",
    "loss: A Tensor containing the value to minimize or a callable taking no arguments which returns the value to minimize. When eager execution is enabled it must be a callable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jcrBNbOFwsPk"
   },
   "outputs": [],
   "source": [
    "gradients = optimizer.compute_gradients(cost)\n",
    "capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Padding\n",
    "Before training, we work on the dataset to convert the variable length sequences into fixed length sequences, by padding. We use a few special symbols to fill in the sequence.\n",
    "\n",
    "\n",
    "PAD : Filler\n",
    "\n",
    "GO : Start decoding\n",
    "\n",
    "UNK : Unknown; word not in vocabulary\n",
    "\n",
    "Consider the following query-response pair.\n",
    "\n",
    "Q : How are you?\n",
    "A : I am fine.\n",
    "\n",
    "Assuming that we would like our sentences (queries and responses) to be of fixed length, 10, this pair will be converted to:\n",
    "\n",
    "Q : [ PAD, PAD, PAD, PAD, PAD, PAD, “?”, “you”, “are”, “How” ]\n",
    "\n",
    "A : [ GO, “I”, “am”, “fine”, “.”, EOS, PAD, PAD, PAD, PAD ]\n",
    "\n",
    "EOS : End of sentence\n",
    "\n",
    "The result of the padding sequences is pretty straight forward. You can now observe that the list of sentences that have been padded out into a matrix where each row in the matrix has an encoded sentence with the same length.Its computationaly expensive as we work with large dataset there we have bucketing techinique but its not applied for the moment.\n",
    "\n",
    "\n",
    "# Bucketing\n",
    "Introduction of padding did solve the problem of variable length sequences, but consider the case of large sentences. If the largest sentence in our dataset is of length 100, we need to encode all our sentences to be of length 100, in order to not lose any words. Now, what happens to “How are you?” ? There will be 97 PAD symbols in the encoded version of the sentence. This will overshadow the actual information in the sentence.\n",
    "\n",
    "Bucketing kind of solves this problem, by putting sentences into buckets of different sizes. Consider this list of buckets : [ (5,10), (10,15), (20,25), (40,50) ]. If the length of a query is 4 and the length of its response is 4 (as in our previous example), we put this sentence in the bucket (5,10). The query will be padded to length 5 and the response will be padded to length 10. While running the model (training or predicting), we use a different model for each bucket, compatible with the lengths of query and response. All these models, share the same parameters and hence function exactly the same way.\n",
    "\n",
    "If we are using the bucket (5,10), our sentences will be encoded to :\n",
    "\n",
    "Q : [ PAD, “?”, “you”, “are”, “How” ]\n",
    "\n",
    "A : [ GO, “I”, “am”, “fine”, “.”, EOS, PAD, PAD, PAD, PAD ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hy8u3-bjwsPo"
   },
   "outputs": [],
   "source": [
    "def pad_sentence(sentence_batch, pad_int):\n",
    "    padded_seqs = []\n",
    "    seq_lens = []\n",
    "    max_sentence_len = max([len(sentence) for sentence in sentence_batch])\n",
    "    for sentence in sentence_batch:\n",
    "        padded_seqs.append(sentence + [pad_int] * (max_sentence_len - len(sentence)))\n",
    "        seq_lens.append(len(sentence))\n",
    "    return padded_seqs, seq_lens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy computation\n",
    "<b>np.pad</b> will  take the input array and add the padding based on the shape \n",
    "\n",
    "<b>np.equal</b>compare the target and prediction elementwise\n",
    "\n",
    "<b>np.mean</b>Average of the inputs given\n",
    "\n",
    "    target = [1, 2, 3, 4, 5]\n",
    "    print(np.pad(target, (2, 3), 'constant', constant_values=(4, 6)))\n",
    "    output: [4 4 1 2 3 4 5 6 6 6]\n",
    "    target='Iam good'=[1 5]___length=2,  logits='Iam doing good'=[1 7 5]___length=3\n",
    "    if max_seq=3  then target=[1 5 0]   (After np.pad-Pad an array.)\n",
    "                       logits=[1 7 5]\n",
    "    take mean to get average accuracy of all batch:\n",
    "        compare sentence using   np.equal([1, 5, 0], [1, 7 ,5])           output:[ True False False]\n",
    "        mean of the result using np.mean(np.equal([1, 5, 0], [1, 7 ,5]))) output:0.3333333333\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SgVkMCsmwsPq"
   },
   "outputs": [],
   "source": [
    "def get_accuracy(target, logits):\n",
    "    max_seq = max(len(target[1]), logits.shape[1])\n",
    "    if max_seq - len(target[1]):\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - len(target[1]))],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train and Test data split\n",
    "As we know input and output will be our questions and answers.Here we are splliting our dataset wrt batch size(128)\n",
    "\n",
    "ex:Questions from 1 to 128  index in questions_int list will be our validation traing set and 128 to the end of list will be our train data its because as we need less data for validation than training\n",
    "\n",
    "Same goes for test data from answers_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V8oeeDm-wsPr"
   },
   "outputs": [],
   "source": [
    "train_data = questions_int[batch_size:]\n",
    "test_data = answers_int[batch_size:]\n",
    "val_train_data = questions_int[:batch_size]\n",
    "val_test_data = answers_int[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "MQ-XF1liwsPu",
    "outputId": "40994dc8-d3b9-4e62-ba9c-3a05e922628a"
   },
   "outputs": [],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare train test validation set\n",
    "\n",
    "we need to pad the sentence before use it to validate our model as we seen in padding section.\n",
    "As our vocabulary index already has the word 'PAD' it.checking it us:\n",
    "\n",
    "vocabs_to_index['<PAD>']--->6283(index where its located)\n",
    "    \n",
    "So padding the sentence if max length of sentence in vocab index is 4 as ex:\n",
    "\n",
    "    ['how','are','you']=[0, 1, 2]                 -Before padding\n",
    "    ['how','are','you','']=[0, 1, 2,6283]         -After padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tsSesPuRwsPw"
   },
   "outputs": [],
   "source": [
    "pad_int = vocabs_to_index['<PAD>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9-KGHuRrwsPz"
   },
   "outputs": [],
   "source": [
    "val_batch_x,val_batch_len = pad_sentence(val_train_data,pad_int)\n",
    "val_batch_y,val_batch_len_y = pad_sentence(val_test_data,pad_int)\n",
    "val_batch_x = np.array(val_batch_x)\n",
    "val_batch_y = np.array(val_batch_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Round the length of train data \n",
    "we need to round the length of train data wrt batch size in order to have equal number of sentence in each batch \n",
    "\n",
    "For ex: \n",
    "        \n",
    "     we have length of train data=103 and our bacth size=10\n",
    "        103/10=10.3 \n",
    "     we cant have 10.3 data so we need to round sentence in batch to 10 so now we must get the rounded train data to obtain the same for whole training set .its done as follow:\n",
    "        10*10=100\n",
    "     So our round length of train data is 100 we dont care about the the 3 sentence which is left \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CzX0zA7zwsP2"
   },
   "outputs": [],
   "source": [
    "no_of_batches = math.floor(len(train_data)//batch_size)\n",
    "round_no = no_of_batches*batch_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentence to sequence\n",
    "So as given below if we have a question sentence 'how are you' it must not be given as it is in our rnn it must be converted into vector\n",
    "\n",
    "Ex:\n",
    "    \n",
    "    'how are you' to  [0, 1, 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentence_to_seq(sentence, vocabs_to_index):\n",
    "    results = []\n",
    "    for word in sentence.split(\" \"):\n",
    "        if word in vocabs_to_index:\n",
    "            results.append(vocabs_to_index[word])\n",
    "        else:\n",
    "            results.append(vocabs_to_index['<UNK>'])        \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question_sentence = 'where are you'\n",
    "question_sentence = sentence_to_seq(question_sentence, vocabs_to_index)\n",
    "print(question_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tf Session Run/Train Model\n",
    "Only after running <b>tf.global_variables_initializer()</b> in a session our variables hold the values we told them to hold when we declare them (tf.Variable(tf.zeros(...)), tf.Variable(tf.random_normal(...)),...).\n",
    "\n",
    "From the TF doc :\n",
    "    \n",
    "    \n",
    "    Calling tf.Variable() adds several ops to the graph:\n",
    "    A variable op that holds the variable value.An initializer op that sets the variable to its initial value. This is actually a tf.assign op.The ops for the initial value, such as the zeros op for the biases variable in the example are also added to the graph.\n",
    "\n",
    "    And also:Variable initializers must be run explicitly before other ops in your model can be run. The easiest way to do that is to add an op that runs all the variable initializers, and run that op before using the model.\n",
    "    \n",
    "### tqdm:\n",
    "\n",
    "    for bs in tqdm(range(0,round_no  ,batch_size)):\n",
    "    tqdm() takes bs(batch) and iterates over it, but each time it yields a new bs (between each iteration of the loop), it also updates a progress bar in out output cell. \n",
    "    \n",
    "### Session\n",
    "we need to run the session by providing the optimize which compute gradient wrt cost in each step and all input and target data  with its length.\n",
    "It can return <b>prediction</b> of input data and it must be compared with the original target data to get the accuracy of our model.and which will be cumulated further to get total accuracy for all the batches in a single epochs.\n",
    "Also return the <b>loss</b> for each batch which will be cumulated further to get total loss for all the batches in a single epochs\n",
    "\n",
    "<b>optional:</b>Also in each epoch the current tf session  takes the input_sentence(question) we assigned before such as 'how are you' as input and gives the prediction (answers).It can help us to review how much the predicted answers are releent to the reponse of human reply."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#file_writer = tf.summary.FileWriter('D:/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#summaries_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51017
    },
    "colab_type": "code",
    "id": "3T3wGVUEwsP5",
    "outputId": "aa9a1f50-72eb-4fa1-b1f5-cddfec4376e0",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "save_path = '/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/model_weights'\n",
    "acc_plt = []\n",
    "loss_plt = []\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for epoch in range(epochs):\n",
    "        #_, summaries_str = sess.run([train_op, summaries_op])\n",
    "        #fw.add_summary(summaries_str, global_step=i)\n",
    "        total_accuracy = 0.0\n",
    "        total_loss = 0.0\n",
    "        for bs in tqdm(range(0,round_no  ,batch_size)):\n",
    "          index = min(bs+batch_size, round_no )\n",
    "          #print(bs,index)\n",
    "      \n",
    "          #padding done seperately for each batch in training and testing data\n",
    "          batch_x,len_x = pad_sentence(train_data[bs:index],pad_int)\n",
    "          batch_y,len_y = pad_sentence(test_data[bs:index],pad_int)\n",
    "          batch_x = np.array(batch_x)\n",
    "          batch_y = np.array(batch_y)\n",
    "        \n",
    "          pred,loss_f,opt = sess.run([inference_logits,cost,train_op], \n",
    "                                      feed_dict={input_data:batch_x,\n",
    "                                                target_data:batch_y,\n",
    "                                                input_data_len:len_x,\n",
    "                                                target_data_len:len_y,\n",
    "                                                lr_rate:learning_rate,\n",
    "                                                keep_prob:0.75})\n",
    "\n",
    "          train_acc = get_accuracy(batch_y, pred)\n",
    "          total_loss += loss_f \n",
    "          total_accuracy+=train_acc\n",
    "    \n",
    "        total_accuracy /= (round_no // batch_size)\n",
    "    \n",
    "        total_loss /=  (round_no//batch_size)\n",
    "        acc_plt.append(total_accuracy)\n",
    "        loss_plt.append(total_loss)\n",
    "        prediction_logits = sess.run(inference_logits, {input_data: [question_sentence]*batch_size,\n",
    "                                         input_data_len: [len(question_sentence)]*batch_size,\n",
    "                                         target_data_len: [len(question_sentence)]*batch_size,              \n",
    "                                         keep_prob: 0.75,\n",
    "                                         })[0]\n",
    "        print('Epoch %d,Average_loss %f, Average Accucracy %f'%(epoch+1,total_loss,total_accuracy))\n",
    "        print('  Inputs Words: {}'.format([index_to_vocabs[i] for i in question_sentence]))\n",
    "        print('  Replied Words: {}'.format(\" \".join([index_to_vocabs[i] for i in prediction_logits])))\n",
    "        print('\\n')\n",
    "        saver = tf.train.Saver() \n",
    "        saver.save(sess, save_path)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "WkzXCp_swsP9",
    "outputId": "1ba662bb-8c48-4349-bb68-2635ccb58412"
   },
   "outputs": [],
   "source": [
    "#Accuracy vs Epochs\n",
    "plt.plot(range(epochs),acc_plt)\n",
    "plt.title(\"Change in Accuracy\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "colab_type": "code",
    "id": "HcbTo1j0y9my",
    "outputId": "de08d92a-2dfe-4c31-fcaf-f186572108c7"
   },
   "outputs": [],
   "source": [
    "#loss vs Epochs\n",
    "plt.plot(range(epochs),loss_plt)\n",
    "plt.title(\"Change in loss\")\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Lost')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VwUlywN4Ajzl"
   },
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "39MSfl6AAlTS"
   },
   "outputs": [],
   "source": [
    "pickle.dump(acc_plt,open('accuracy.p','wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XuRLcdV-BJQF"
   },
   "outputs": [],
   "source": [
    "pickle.dump(loss_plt,open('loss.p','wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get tokens:\n",
    "Get all the codes/tokens we additionaly added in the vocab dictionary\n",
    "\n",
    "    6283 '<PAD>'\n",
    "    6284 '<EOS>'\n",
    "    6285 '<UNK>'\n",
    "    6286 '<GO>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "Z9tjyWtGtorR",
    "outputId": "43d99274-29ef-4842-a837-ccac6f6bdc80"
   },
   "outputs": [],
   "source": [
    "#get all the codes/tokens we additionaly added in the vocab dictionary\n",
    "garbage = []\n",
    "for code in codes:\n",
    "  print(vocabs_to_index[code])\n",
    "  garbage.append(vocabs_to_index[code])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare the questions,Answers, and predicted answers\n",
    "In order to prepare the questions and human answer  and bot answer we need to clean the sentence by removing the token we discussed in the previous step.\n",
    "    \n",
    "    if we get the token of <EOS> we break the loop \n",
    "    if the word is not the one of the additional token <PAD>,<UNK>,<GO> then we wont consider these words.we must return the data without these tokens "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5kv4x-p94Rvk"
   },
   "outputs": [],
   "source": [
    "#prepare the question,answer and prediction data\n",
    "def print_data(i,batch_x,index_to_vocabs):\n",
    "  data = []\n",
    "  for n in batch_x[i]:\n",
    "    if n==garbage[1]:\n",
    "      break\n",
    "    else:\n",
    "      if n not in [6283,6285,6286]:\n",
    "        data.append(index_to_vocabs[n])\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bC1ih2B396Bm"
   },
   "outputs": [],
   "source": [
    "ques = []\n",
    "real_answer = []\n",
    "pred_answer = []\n",
    "for i in range(len(val_batch_x)):\n",
    "  ques.append(print_data(i,batch_x,index_to_vocabs))\n",
    "  real_answer.append(print_data(i,batch_y,index_to_vocabs))\n",
    "  pred_answer.append(print_data(i,pred,index_to_vocabs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Printing Real and predicted Answers\n",
    "So from the below output we can comes to a conclusion how well our trained model works with the validation set(size of batch_size i.e 128 rows) that we have created earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 10897
    },
    "colab_type": "code",
    "id": "QvUS1zq3_zqL",
    "outputId": "2c12f491-01cd-4486-b0af-00993da6780d"
   },
   "outputs": [],
   "source": [
    "for i in range(len(val_batch_x)):\n",
    "    print('row %d'%(i+1))\n",
    "    print('QUESTION:',' '.join(ques[i]))\n",
    "    print('REAL ANSWER:',' '.join(real_answer[i]))\n",
    "    print('PREDICTED ANSWER:',' '.join(pred_answer[i]),'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load save model for given sample test\n",
    "As here i have assigned my sentence to the variable \n",
    "            \n",
    "            question_sentence_2\n",
    "Initialize tf.session with the graph and load the model meta graph from the saved path to use the model for prediction.\n",
    "as earlier describe the iput for session run.So as we used differeznt keep_prob (dropout for regularization).Earlier i used 0.5 and now its 1.0 in order to see the better performance but its also difficult to conclude this hyperparameter value using this single example.Its for the example purpose.\n",
    "\n",
    "As we dont know the target length for the moment i assign the maximum length\n",
    "\n",
    "Finally out put the word id from vocab dictionary with the corresponding sentence for question and answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "colab_type": "code",
    "id": "30qfB77zEMx8",
    "outputId": "457a6348-e21c-4d52-efe6-74143ec48e59"
   },
   "outputs": [],
   "source": [
    "question_sentence_2 = 'what are you doing?'\n",
    "question_sentence_2 = sentence_to_seq(question_sentence_2, vocabs_to_index)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(save_path + '.meta')\n",
    "    loader.restore(sess, save_path)\n",
    "\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    input_data_len = loaded_graph.get_tensor_by_name('input_len:0')\n",
    "    target_data_len = loaded_graph.get_tensor_by_name('target_len:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "\n",
    "    prediction_logits = sess.run(logits, {input_data: [question_sentence_2]*batch_size,\n",
    "                                         input_data_len: [len(question_sentence_2)]*batch_size,\n",
    "                                         target_data_len : [5]*batch_size,\n",
    "                                         keep_prob: 1.0})[0]\n",
    "\n",
    "print('Input')\n",
    "print('  Word Ids:      {}'.format([i for i in question_sentence_2]))\n",
    "print('  Question: {}'.format([index_to_vocabs[i] for i in question_sentence_2]))\n",
    "\n",
    "print('\\nPrediction')\n",
    "print('  Word Ids:      {}'.format([i for i in prediction_logits]))\n",
    "print('  Answer: {}'.format(\" \".join([index_to_vocabs[i] for i in prediction_logits])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Even history to use with Tensorboard\n",
    "Save the graph event file for to visualize the graph computation in tensorboard\n",
    " use the command below within relative path in the tensorflow\n",
    " \n",
    "     env tensorboard --logdir=Notebook\\model_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_writer = tf.summary.FileWriter('D:/ML Projects/Global IA/Seq2Seq-Chatbot/Notebook/model_weights/log', sess.graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the learned embeddings\n",
    "\n",
    "Next, let's retrieve the word embeddings learned during training. This will be a matrix of shape (vocab_size, embedding-dimension).\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "We will now write the weights to disk. To use the Embedding Projector, we will upload two files in tab separated format: a file of vectors (containing the embedding), and a file of meta data (containing the words).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = model.layers[0]\n",
    "weights = e.get_weights()[0]\n",
    "print(weights.shape) # shape: (vocab_size, embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "\n",
    "encoder = info.features['text'].encoder\n",
    "\n",
    "out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n",
    "out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n",
    "\n",
    "for num, word in enumerate(encoder.subwords):\n",
    "  vec = weights[num+1] # skip 0, it's padding.\n",
    "  out_m.write(word + \"\\n\")\n",
    "  out_v.write('\\t'.join([str(x) for x in vec]) + \"\\n\")\n",
    "out_v.close()\n",
    "out_m.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "The training data on Cornell Movie Subtitle corpus produced a result that needs further improvement \n",
    "and more attention and speculation on training parameters. Adding more quality data will further \n",
    "improve performance. Also, the training model should be trained with other hyper-parameters and \n",
    "different datasets for further experimentation. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "[1] Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation:https://arxiv.org/abs/1406.1078\n",
    "\n",
    "[2] Sequence to Sequence Learning with Neural Networks:https://arxiv.org/abs/1409.3215\n",
    "\n",
    "[3] Neural Machine Translation by Jointly Learning to Align and Translate:https://arxiv.org/abs/1409.0473\n",
    "\n",
    "[4] A Neural Conversational Model:https://arxiv.org/abs/1506.05869\n",
    "\n",
    "[6] Sequence-to-Sequence learning and Neural Conversation model 2017/08/02 \n",
    "https://isaacchanghau.github.io/2017/08/02/Seq2Seq-Learning-andNeuralConversationalModel \n",
    "\n",
    "[7] A Formalization of a Simple Sequential Encoder-Decoder https://mc.ai/a-formalization-of-a-simple-sequential-encoder-decoder/ \n",
    "\n",
    "[8] Neural Machine Translation by Jointly Learning to Align and Translate Dzmitry Bahdanau, Kyunghyun Cho, Yoshua Bengio (Submitted on 1 Sep 2014 (v1), last revised 19 May 2016 (this version, v7)) \n",
    "\n",
    "[9] Dataset collect and information about Cornell movie dialog corpus dataset available at https://www.cs.cornell.edu/ cristian/CornellMovieDialogsCorpus.htm \n",
    "\n",
    "[10]  I. N. d. Silva, D. H. Spatti, R. A. Flauzino, L. H. B. Liboni, and S. F. d. R. Alves, Artificial Neural  \n",
    "Networks A Practical Course, Springer International Publishing, 2017.\n",
    "\n",
    "[11]  O. Davydova, \"7 Types of Artificial Neural Networks for Natural Language Processing,\" \n",
    "[Online].  \n",
    "Available: https://www.kdnuggets.com/2017/10/7-types-artificial-neural-networks-natural language-processing.html.  \n",
    "\n",
    "[12]  G. M and D. S. [Online]. Available:  https://www.sciencedirect.com/science/article/pii/S1352231097004470. \n",
    "\n",
    "[13]  T. Young, D. Hazarika, S. Poria, and E. Cambria, \"Recent Trends in Deep Learning-Based Natural  Language Processing\". \n",
    "\n",
    "[14]  R. Collobert and J. Weston, \"A unified architecture for natural language processing: deep neural networks with multitask learning,\" in Proceedings of the 25th international conference on machine learning, 2008. \n",
    "\n",
    "[15] J¨org Tiedemann. News from OPUS - A collection of multilingual parallel corpora with tools and interfaces. In N. Nicolov, K. Bontcheva, G. Angelova, and R. Mitkov, editors, Recent Advances in Natural Language Processing, volume V, pages 237–248. John Benjamins, Amsterdam/Philadelphia, Borovets, Bulgaria, 2009. \n",
    "\n",
    "[16] Ashish Vaswani, Noam Shazier, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. CoRR, abs/1706.03762, 2017. \n",
    "\n",
    "[17] Oriol Vinyals and Quoc V. Le. A neural conversational model. CoRR, abs/1506.05869, 2015. \n",
    "\n",
    "[18] Joseph Weizenbaum. Eliza: a computer program for the study of natural language communication between man and machine. Commun. ACM, 9(1):36–45, January 1966\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "new_chatbot_checkling.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
